# -*- coding: utf-8 -*-
"""Machine_Learning_Walkthrough.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fRobX5bXlgUGZDcgpNpLtneZPXHWkG-s

#Machine Learning Walk-Through

#Overview

Machine Learning can be a tricky subject to tackle. Loosely described, it is the science of programming computers to learn from data *without being explicitly programmed* what to learn.

As a result, different Machine Learning algorithms allow for different patterns and variations, which themselves are dependent on the data fed to them. In this notebook, we will take a short overview into the Machine Learning field and describe various algorithms, segmented into **Regressors**, **Classifiers**, and **Clusterings**. 

> The purpose of this notebook will be as a refresher to some ML algorithms, and eventually progress to an all-encompassing notebook on ML.

❗❗❗
##But Why Learn Machine Learning?
Machine Learning allows for: 

*   Easier, shorter, simpler programming with increasing accuracy
*   Allows for program adjustments to new rules and patterns
* Allows for novel approaches to complex problems
* **Can help discover patterns that were not immediately apparent in large amounts of data**

## Challenges of Machine Learning
However, Machine Learning is not an all-encompassing solution. Some challenges include: 

- Insufficient Quantity of Training Data

  > ML Algorithms require a lot of data to train and prepare themselves on before being tested. However, the training data must also represent the problem we are trying to generalize. Insufficient data can lead to sampling bias and *scewed results*

- Poor Quality of Data

  > ML Algorithms are heavily delicate, and thus require extremely *clean* data to learn on. This is data filled with *errors*, *outliers*, and *noise*.

- Irrelevant Features

  > ML Algorithms are only so good at learning. If you feed it garbage in, it will return garbage. Thus, just as importantly, *feature engineering* is just as important in preprocessing as implementing the algorithm

- Overfitting and Underfitting

  > Overfitting occurs when the ML model performs well on training data, but does not generalize well. Underfitting, conversely, means that the model is too simple to learn the underlying structure and nature of the data provided. Solutions may include *gathering more data* or *feeding a better model*

#Libraries

First, we need to import our required libraries and packages in order to properly analyze our dataset and understand what we are working with. What we are using is as follows: 
- Pandas is a python library used explicitly for data visualization and analysis
- Matplotlib is a python library used for data visualization and graphing
- Seaborn is a is a python library used for making statistical graphics
- Sklearn is a machine learning library for which we will use to analyze our data. From it, we can use algorithms such as 
  - Support Vector Machines (or SVMs)
  - Decision Tree Regressors and Classifiers
  - Random Forrest Regressors
  - Linear Regression
  - Logistic Regression
  - K Neigbors Classifier
  - KMeans
  - Splitting our data into training and testing sections 
  - Calculating accuracy and other metrics of grading Machine Learning Algorithms as listed above
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import pylab as pl
import numpy as np
from math import sqrt
from mpl_toolkits.mplot3d import Axes3D 
import matplotlib.pyplot as plt
import matplotlib.cm as cm
# %matplotlib inline
import itertools
import random 
import scipy
import pylab
from sklearn import linear_model
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures
from sklearn import preprocessing
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import log_loss
from sklearn.metrics import classification_report, confusion_matrix
from sklearn import svm 
from sklearn import manifold, datasets 
from sklearn.cluster import KMeans
from sklearn.metrics import f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix, f1_score
from sklearn.metrics import precision_recall_curve, precision_score, recall_score, roc_curve, roc_auc_score
from scipy.optimize import curve_fit
import warnings
warnings.filterwarnings('ignore')
import seaborn as sns
from plotly import express as px, graph_objects as go
from plotly.subplots import make_subplots

"""#Visualization Demonstration

Before jumping in, we are going to demonstrate some best practices of examining and visualizing our data in order to understand it better.

In order to do this, we must first load our data, received as a "raw" file from a github link in a csv format, and loaded in as df_CO2.
"""

raw_file = "https://raw.githubusercontent.com/H-Jan/ML_Walkthrough/main/Data/CO2_emission.csv"
df_CO2 = pd.read_csv(raw_file)

"""Now we can glance into our data and mentally extrapulate features. Upon first look we note: 
- 12 columns, each representing a feature
- 8/12 columns are numerical values, of which 3 are integers and 5 are floats
- Features are
  - Model Year
  - Make
  - Model
  - Vehicle Class
  - Engine Size
  - Cylinders
  - Transmission
  - Fuel Consumption in City
  - Fuel Consumption on the Highway
  - Fuel Consumption Combined
  - CO2 Emissions
  - Smog Level
  
"""

df_CO2.head()

"""Looking at the shape, it confirms our 12 column feature assessment above, and describes to us the size of the dataset. Presented as a tuple, the below shape indicates
> 935 values of features

> 12 different features
"""

df_CO2.shape

"""Diving deeper into the quantitative information, we note that the data type, or *Dtype* lists our data as integers, floats, or objects
- **Note** often describe written values in a dataset

We can also note that there are non-null values in our dataset, meaning there are values or *inputs* for all our features. This is indicative of a *clean dataset*




"""

df_CO2.info()

"""However, we must still double check to ensure that all values are filled, and there are no null-values"""

pd.isnull(df_CO2)

"""Now we can analyze the data feature by feature to see the details of our dataset. Understanding the background from which the dataset originated is important in analyzing, preprocessing, and interpreting the data.

Sinze our focus is on understanding relationships of select features and their affects on CO2 Emissions, we will dive deeper into the following features
 - Vehicle Class
  > The larger the class of vehicle, assumably the larger amount of CO2 Emissions produced. 
 - Cylinders
 > The more amount of cylinders, we can assume a larger amount of CO2 Emissions produced
 - Model Year
 > The newer a vehicle may be, the likelier the vehicle will be efficient, and thus produce less CO2 Emissions


**Note:** A key part of interpreting data and raising critical questions is *understanding* the data itself and the *context* of the question.

Below we take a look at our features and visualize their distributions in our dataset
"""

df_CO2['CO2_Emissions'].value_counts()

fig = px.histogram(df_CO2, x="CO2_Emissions")
fig.update_layout(title_text='Distribution of Vehicles based on CO2 Emissions in Grams PPM',xaxis_title="CO2_Emissions",yaxis_title="Number of Vehicle" )
fig.show()

df_CO2['Engine_Size'].value_counts()

fig = px.histogram(df_CO2, x="Engine_Size")
fig.update_layout(title_text='Distribution of Vehicles based on Displacement',xaxis_title="Engine Size",yaxis_title="Number of vehicles " )
fig.show()

df_CO2['Cylinders'].value_counts()

fig = px.histogram(df_CO2, x="Cylinders")
fig.update_layout(title_text='Distribution of Vehicles based on Amount of Cylinders',xaxis_title="Amount of Cylinders",yaxis_title="Number of vehicles " )
fig.show()

df_CO2['Vehicle_Class'].value_counts()

fig = px.histogram(df_CO2, x="Vehicle_Class")
fig.update_layout(title_text='Distribution of Vehicles by Class',xaxis_title="Vehicle Class",yaxis_title="Number of vehicles " )
fig.show()

df_CO2['Model_Year'].value_counts()

fig = px.histogram(df_CO2, x="Model_Year")
fig.update_layout(title_text='Distribution of Vehicles based on Model Year',xaxis_title="Vehicle Model Year",yaxis_title="Number of vehicles " )
fig.show()

"""Now that we've explored the data in a qualitative manner, we can focus on more quantitative analysis. Below, we see statistical anaylsis of our dataset, including key components such as: 
- Mean
- Standard Deviation
- Interquartile Ranges



"""

df_CO2.describe()

"""Before continuing onto our ML model, we want to create a correlation map to see whether any feature is highly correlated to another feature. 

Highly correlated features would show extreme colors, with a 1 representing a perfect correlation and a deepened red, and a 0 demonstrating no correlation and a light vanilla. The only features with a perfect correlation are those compared against themselves (as expected).

Indicative of the usability of this dataset, many features are highly correlated, allowing for easy extrapolation for a simple ML algorithm to derive patterns. 
"""

correl = df_CO2.corr()
heatmap = sns.heatmap(correl, annot=True, cmap='OrRd')

heatmap.set_title('Vehicle Heatmap', fontdict={'fontsize': 18}, pad=12);

"""#Regressions Section

## 1. Linear Regression

**Linear Regression** is a common ML Algorithm used to describe the *relationship between a dependent and independent variable*. Used as a form of predictive analysis, the idea of using a Linear Regression algorithm is to extrapolate if a set of features are "related" to each other. This is done through fitting a linear equation to the observed data and relating the weights of individuals to their heights as represented by the dependent variable.

> **Simplified**: Does one feature do a good job in predicting the outcome of another, dependent feature? And if so, *how strong* is said feature in predicting the dependent feature?

❗How do we Analyze Linear Regression? 

Two common ways of analyzing Linear Regression is through the use of **R2 Scores** and **RMSE** or Root Mean Squared Error

> **R2 Score** represents the proportion of the variance in the dependent variable which is predictable from the independent variable or variables. 
- Essentially, the higher the R2 score, *the more correlated the two variables are, and the less the variance*

> **RMSE** represents the distance between the predicted values from the model and the actual values in the dataset. This is calculated mathematically as the square root of the sum of variance of the residual values.
- Essentially, the *lower RMSE value, the better a given model is able to fit a dataset.*

**Question to Answer**: Does Engine Size affect CO2 Emissions?
"""

selected_df_CO2 = df_CO2[['Fuel_Consumption_comb(L/100km)', 'CO2_Emissions', 'Engine_Size', 'Cylinders']]

selected_df_CO2.describe()

"""Looking below, we can visualize and reasonably guess the different relationships between selected values. In particular, Engine Size and CO2 Emissions demonstrate a loose correlation, but this must be mathematically confirmed with Linear Regression. """

sns.pairplot(selected_df_CO2)

sns.set_palette('colorblind')
sns.scatterplot(y = 'CO2_Emissions', x = 'Fuel_Consumption_comb(L/100km)', data = selected_df_CO2)

sns.set_palette('colorblind')
sns.scatterplot(y = 'CO2_Emissions', x = 'Engine_Size', data = selected_df_CO2)

sns.set_palette('colorblind')
sns.scatterplot(y = 'CO2_Emissions', x = 'Cylinders', data = selected_df_CO2)

"""Before training and building our Linear Regression model, we preprocess our data using **MinMaxScaler**. The purpose of MinMaxScaler is to transform our features and *scale* them to a given range. 

❗Why is this done?

Implementing MinMaxScaler allows our features to be equally weighed and assessed by our ML Algorithm by *preserving the shape of our original dataset*. Notably, it does **not** change the information of the original data. 

>**Simplified**: MinMaxScaler keeps the *shape* of our data, and thus the *relationship* of each data point relative to other data points. As a result, all our numbers are condensed from 0 to 1, but the data itself and it's hidden patterns remain the same. 


"""

scaler = MinMaxScaler()
df_scaled = selected_df_CO2[['Engine_Size', 'CO2_Emissions', 'Cylinders', 'Fuel_Consumption_comb(L/100km)']]
df_scaled = pd.DataFrame(scaler.fit_transform(df_scaled), columns = df_scaled.columns)

"""Now, we split our data into two sets: **training** and **testing** to help set up our Linear Regression Model. This allocates certain ratio of our dataset to exclusively training our algorithm and another portion to exclusively testing our algorithm.  
- **Training** is used to train our algorithm so that it can figure out the patterns of our dataset
- **Testing** is used to test how well our algorithm figured out patterns by giving it new data and seeing how well it's learned pattern fits the unsees data
"""

X = df_scaled[['Engine_Size', 'Cylinders', 'Fuel_Consumption_comb(L/100km)']]
y = df_scaled['CO2_Emissions']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)

"""Now, we can finally apply our Linear Regression model by fitting it to our trained data, and evaluating how well it predicts on our testing data. """

LR = LinearRegression().fit(X_train, y_train)

y_predict = LR.predict(X_test)

LR_score = r2_score(y_test, y_predict)
LR_rmse = mean_squared_error(y_test, y_predict, squared=False)
print("Coefficients: ", LR.coef_)
print("Intercept: ", LR.intercept_)
print("R2 Score : ", LR_score)
print("RMSE : ", LR_rmse)

"""❗How Do We Evaluate This Score?

The **R2 score**, as mentioned above, represents how closely related two variables are according to our given algorithm. A score of 0.943 indicates: 
- A strongly, positive correlation between the two variables tested

>**General Rule of Thumb**: 
  - A positive value indicates a positive relationship between two variables 
  - A negative value indicates a negative, or inverse relationship between two values 
  - Values over 0.75 are considered strongly correlated, and vice versa for negative values

Moreover, the presence of a low **RMSE** value indicates that our Linear Regression formula generated by the computer *fits* very well, as indicated by the *low, fractional* value. This is a good measure of how *accurately* our model predicts relationships between the two values on our testing data.

As a result, we can firmly answer our initial question: **yes**, there is a strong relationship between Engine Size and CO2 Emissions

To better visualize the Linear Regression formula, below is an example of the line fitment on our chosen variables. 
"""

sns.regplot(x="Engine_Size", y="CO2_Emissions", data=df_CO2)

"""##Decision Tree Regression

**Decision Tree Regressor** is a ML Algorithm used to predict the value of a variable by learning simple decision rules. The purpose of a Decision Tree Regressor is to minimize the **RMSE** by essentially splitting down the data to better fit a model. 

❗How Does it Work? 

Surpsingly, a Decision Tree Regressor is rather intuitive. It operates by finding points in the independent variable to split the data into two parts, so that the **RMSE** (as described above) lowers. When done enough times, the visual is tree-like, hence it's name

> **Simplified**: Think of a Decision Tree Regressor as splitting firewood to fit an oddly shaped firepit. In our Linear Regression model, we had chunky firewood that fit pretty well in our oddly shaped firepit. But sometimes we need to fit the firewood better into our firepit so that the gaps left in between (RMSE) are minimized. 
  - So, when possible, the computer "splits" our firewood down so that the firewood fits better and the "gap" (RMSE) is minimized. 

❗How Do We Evaluate Our Model?

We evaluate a **DTR** model following the same metrics as a Linear Regression model.
"""

DTR = DecisionTreeRegressor().fit(X_train, y_train)

y_predict = DTR.predict(X_test)

DTR_score = r2_score(y_test, y_predict)
DTR_rmse = mean_squared_error(y_test, y_predict, squared=False)
print("R2 Score is: ", DTR_score)
print("RMSE is: ", DTR_rmse)

"""**Results**: As seen above, our Decision Tree Regressor better models the relationship between Engine Size and CO2 Emissions, producing a *higher R2 score* (better fitting model) and a *lower RMSE value*. 

R2 Score: 0.946 
RMSE Value: 0.0279

## Random Forest Regression

**Random Forest Regression** is a ML algorithm that uses **ensemble learning**, or multiple algorithms at once, on Decision Tree's, while adding a bit of randomness to the model while growing the tree to better fit the data.

❗How Does it Work? 

> **Simplified**: If you think one decision tree regressor is good, this is a whole team of them, working together, and eventually merged for even better statistical values.

❗How Do we Evaluate our RFR Model? 

Since this model is a simple combination of **DTRs**, we evaluate by the same metrics.
"""

RFR = RandomForestRegressor().fit(X_train, y_train)

y_predict = RFR.predict(X_test)

RFR_score = r2_score(y_test, y_predict)
RFR_rmse = mean_squared_error(y_test, y_predict, squared=False)
print("R2 Score is: ", RFR_score)
print("RMSE is: ", RFR_rmse)

"""**Results**: As seen above, our Random Forest Regressor better models the relationship between Engine Size and CO2 Emissions, producing a *higher R2 score* (better fitting model) and a *lower RMSE value*.

R2 Score: 0.964 
RMSE Value: 0.0228

###Comparing All 3 Above

Below is a more direct comparison between the above 3 regressors, allowing a more concrete understanding of the improvements from model to model.
"""

table = pd.DataFrame({
    'Names' : ['Linear Regression', 'Decision Tree Regression', 'Random Forrest Regression'],
    'R2 Scores ' : [LR_score, DTR_score, RFR_score],
    'RMSE Values' : [LR_rmse, DTR_rmse, RFR_rmse]
})

table

"""## Non-Linear Regression 

**Non-Linear Regression** is an ML Algorithm that operates in the same principles of a Linear Regression model, but fits a model as expressed in a *non-linear mathematical function*

> **Simplified**: Working similarly in principle to Linear Regression, Non-Linear Regression operates the same way. But sometimes fitting a straight line mathematical equation is difficult when you have crazy data. And thus, instead, we can fit a different mathematical function, such as a logarithm, exponents, powers, or, as seen below, *sigmoid and polynomial functions*

❗How Do we Evaluate our Non-Linear Models?

Since this model is a simple variation of Linear Regression, we evaluate by the same metrics.
"""

df_China = pd.read_csv("https://raw.githubusercontent.com/H-Jan/ML_Walkthrough/main/Data/china_gdp.csv")

"""First, some simple data understanding and processing"""

df_China.head()

"""Now we can visualize the error of applying a Linear Regression model, and the necessity of a Non-Linear Regression model. The data increases exponentially, and thus requires a different mathematical approach. In this case, we will use a **sigmoid** function"""

plt.figure(figsize=(8,5))
x_data, y_data = (df_China["Year"].values, df_China["Value"].values)
plt.plot(x_data, y_data, 'ro')
plt.xlabel('Year')
plt.ylabel('GDP')
plt.title('GDP Growth of China')
plt.show()

"""Now we build our Non-Linear Regression Model. When applied over our data, you can see that it is quite distant from the datapoints, and thus would not evaluate well. This is precisely the step in which *our computer performs it's learned pattern to fit the model on our data*"""

def sigmoid(x, Beta_1, Beta_2):
     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))
     return y

# Lets look at a sample sigmoid line that might fit with the data
beta_1 = 0.10
beta_2 = 1990.0

#logistic function
Y_pred = sigmoid(x_data, beta_1 , beta_2)

#plot initial prediction against datapoints
plt.plot(x_data, Y_pred*15000000000000.)
plt.plot(x_data, y_data, 'ro')

"""As done before, we normalize the data, preserving the shape yet reducing the values to a specified range"""

xdata = x_data/max(x_data)
ydata = y_data/max(y_data)

"""Next we find the best parameters to fit our line"""

popt, pcov = curve_fit(sigmoid, xdata, ydata)
#print the final parameters
print(" beta_1 = %f, beta_2 = %f" % (popt[0], popt[1]))

"""And lastly, we plot our regression model on the data to visualize the computers calculations
- A large improvement on the previous graph
"""

x = np.linspace(1960, 2015, 55)
x = x/max(x)
plt.figure(figsize=(8,5))
y = sigmoid(x, *popt)
plt.plot(xdata, ydata, 'ro', label='data')
plt.plot(x,y, linewidth=3.0, label='fit')
plt.legend(loc='best')
plt.ylabel('GDP')
plt.xlabel('Year')
plt.show()

"""Looks Pretty Good! 
Now let's evaluate the accuracy of our model using the same metrics of **R2 Score** and **RMSE**, done by splitting the data into our training and testing sets as previously done, and having our computer learn and fit our mathematical equation
"""

#Split the Data into train/test 

data_split = np.random.rand(len(df_China)) < 0.8

X_train = xdata[data_split]
X_test = xdata[~data_split]
y_train = ydata[data_split]
y_test = ydata[~data_split]

#Build Out our model with training set 

popt, pcov = curve_fit(sigmoid, X_train, y_train)

#Predictions made with test set
y_hat = sigmoid(X_test, *popt)

print("Mean Absolute Error is: %.2f" % np.mean(np.absolute(y_hat - y_test)))

print("Residual Sum of Squares (MSE) is: %.2f" % np.mean((y_hat - y_test **2)))

print("R2 Score is: %.2f" % r2_score(y_hat, y_test))

"""## Polynomial Regression

Returning back to the CO2 Dataset since it is non-binary dataset (unlike the previous China GDP dataset) and can thus allow the application of polynomial regression, which is a special case of multiple linear regressions, and therefore allows the same approach.

Train Our Dataset
"""

data_split = np.random.rand(len(df_scaled)) < 0.8

train_set = df_CO2[data_split]
test_set = df_CO2[~data_split]

X_train = np.asanyarray(train_set[['Engine_Size']])
y_train = np.asanyarray(train_set[['CO2_Emissions']])

X_test = np.asanyarray(test_set[['Engine_Size']])
y_test = np.asanyarray(test_set[['CO2_Emissions']])

polynomial_reg = PolynomialFeatures(degree=2)
X_train_polynomial = polynomial_reg.fit_transform(X_train)

X_train_polynomial

LR = linear_model.LinearRegression()
new_y_train = LR.fit(X_train_polynomial, y_train)

plt.scatter(train_set.Engine_Size, train_set.CO2_Emissions,  color='blue')
XX = np.arange(0.0, 10.0, 0.1)
yy = LR.intercept_[0]+ LR.coef_[0][1]*XX+ LR.coef_[0][2]*np.power(XX, 2)

plt.plot(XX, yy, '-r' )
plt.xlabel("Engine size")
plt.ylabel("Emission")
plt.title("Polynomial Regression on Original Comparison ScatterPlot")

"""The above polynomial equation was performed with 2 degrees. Now we will apply with 3 degrees, increasing the complexity of our formula to allow more "wiggle room" for our computer to fit the model"""

cubed_polynomial_reg = PolynomialFeatures(degree=3)
X_train_polynomial_cubed = cubed_polynomial_reg.fit_transform(X_train)
y_train_polynomial_cubed = LR.fit(X_train_polynomial_cubed, y_train)

"""This mathematical formula may look complex, but it is nothing more than numbers. Each polynomial formula follows a basic structure of:  

> y = ax + b^2 + c^3 + ...

With each degree added, the complexity of the formula increases according to the same pattern.


"""

print('Coefficients: ', LR.coef_)
print('Intercept: ', LR.intercept_)

plt.scatter(train_set.Engine_Size, train_set.CO2_Emissions, color='blue')
XX = np.arange(0.0, 10.0, 0.1)
yy = LR.intercept_[0] + LR.coef_[0][1]*XX + LR.coef_[0][2]*np.power(XX, 2) + LR.coef_[0][3]*np.power(XX, 3)

plt.plot(XX, yy, '-r')
plt.xlabel('Engine Size')
plt.ylabel('CO2 Emissions')

X_test_polynomial_cubed = cubed_polynomial_reg.fit_transform(X_test)
y_test_polynomial_cubed = LR.predict(X_test_polynomial_cubed)

print("Mean Absolute Error is: %.2f" % np.mean(np.absolute(y_test_polynomial_cubed - y_test)))

print("Residual Sum of Squares (MSE) is: %.2f" % np.mean((y_test_polynomial_cubed - y_test) ** 2))

print("R2 Score is: %.2f" % r2_score(y_test, y_test_polynomial_cubed))

"""**Results**: As evidenced by the large RMSE value and low R2 score, this cubed polynomial regression does not fit as well as our Linear Regression model, but is a great final demonstration of the capabilities of regressor models if used properly.

#Classification Section

In the below section, conversely to Regressors, we will explore various *Classifiers*, or predictive models that label a piece of data according to learned groups.

> **Simplified**: Classifiers give you a *discrete* value output. They tell you either **yes** or **no** within the context of your question
"""

telecom_df = pd.read_csv('https://raw.githubusercontent.com/H-Jan/ML_Walkthrough/main/Data/telecom_cus.csv')

telecom_df.head()

telecom_df.shape

telecom_df.info()

pd.isnull(telecom_df)

telecom_df.describe()

correlation = telecom_df.corr()
plt.figure(figsize=(20,20))

heatmap = sns.heatmap(correlation, annot=True, cmap='OrRd')
heatmap.set_title('Vehicle Heatmap', fontdict={'fontsize': 18}, pad=12);

telecom_df['custcat'].value_counts()

"""## K-Nearest Neighbors

**K-Nearest Neighbors**, or **KNN** is a Machine Learning Algorithm that classifies datapoints within a dataset, estimating the likelihood that a data point will become a member of a created group based on the classification of near data points

> **Simplified**: KNN classifiers operate on the belief that things in close proximity to each other are usually near each other, and creates groups accordingly.

❗How do we Analyze K-Nearest Neighbors Classification? 

- KNNs are judged and scored based on the metric of accuracy and how many divisions and formations of groups generate the highest accuracy (the amount of groups is dictated as *k*)

Before beginning, we first need to convert pandas dataframe, or the way we received our data points, to a numpy array, which is a different way of storing our data so that our algorithm may interpret and form groups
"""

X = telecom_df[['region', 'tenure', 'age', 'marital', 'address', 'income', 'ed', 'employ', 'retire', 'gender', 'reside']].values
x[0:5]

y = telecom_df['custcat'].values
y[0:5]

"""As previously done, we are normalizing our data, preserving the shape and relationships of the data while scaling it down. Furthermore, we can as well see the effects of converting our data type into an array. """

X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))
X[0:5]

"""Now we train, test, and split our data 
> *Noticing a pattern in methodology?* 💡
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

"""Diving into it, we arbitrarily set our k = 3, telling our KNN classifier to classify our data into 3 groups"""

k = 3

#Train Model and Predict

neighbors = KNeighborsClassifier(n_neighbors= k).fit(X_train, y_train)

neighbors

"""Lets see our prediction for the first five values of our test and their respective groups. 
- The first two values are classified into group two
- The last three values are classified into group three
"""

y_hat = neighbors.predict(X_test)
y_hat[0:5]

"""Accuracy Evaluation. Let's see how accurately we can classify data points based on a three group only prediction."""

print("Train Set Accuracy: ", metrics.accuracy_score(y_train, neighbors.predict(X_train)))

print("Test Set Accuracy: ", metrics.accuracy_score(y_test, y_hat))

"""**Results**: Unfortunately our accuracy is quite low. on our training portion, the accuracy is a lowly 0.58, or 58% accurate, and only worsens when tested, decreasing to a 34% accuracy. Let's see if we can improve it by increasing the number of groups we allow our KNN to classify the data points into. """

k = 6

neighbors_new = KNeighborsClassifier(n_neighbors= k).fit(X_train, y_train)

y_hat_6 = neighbors_new.predict(X_test)

print("Train Set Accuracy: ", metrics.accuracy_score(y_train, neighbors_new.predict(X_train)))

print("Train Set Accuracy: ", metrics.accuracy_score(y_test, y_hat_6))

"""**Results**: Unfortunately our accuracy is lower. Instead of arbitrarily guessing groups, let's see if we can improve it by iterating through different amounts of groupings."""

k_iter = 10
mean_accuracy = np.zeros((k_iter-1))
standard_accuracy = np.zeros((k_iter-1))

for n in range(1, k_iter):
  neighbor = KNeighborsClassifier(n_neighbors = n).fit(X_train, y_train)
  y_hat_new = neighbor.predict(X_test)
  mean_accuracy[n-1] = metrics.accuracy_score(y_test, y_hat_new)
  standard_accuracy[n-1]=np.std(y_hat_new==y_test)/np.sqrt(y_hat_new.shape[0])

mean_accuracy

"""Plotting our model accuracy for different numbers of neighbors to visualize the best amount of groupings and it's respective accuracy"""

plt.plot(range(1, 10), mean_accuracy, 'b')
plt.fill_between(range(1,10),mean_accuracy - 1 * standard_accuracy, mean_accuracy + 1 * standard_accuracy, alpha=0.10)
plt.fill_between(range(1,10),mean_accuracy - 3 * standard_accuracy, mean_accuracy + 3 * standard_accuracy, alpha=0.10, color="blue")

plt.legend(('Accuracy', '+/- 1 Standard Deviation', '+/- 3 Standard Deviations'))
plt.xlabel('Number of Neighbors')
plt.ylabel('Accuracy')
plt.tight_layout()
plt.show()

print("The best accuracy was with k =", mean_accuracy.argmax()+1, "with an accuracy of ", mean_accuracy.max())

"""**Results**: Unfortunately our best accuracy was with k = 3 at a test accuracy of 0.344, or 34%. This may be indicative of the dataset itself, and our model. Further elaboration will be done to see if it can be improved, or a different dataset chosen to better demonstrate KNNs.

##Decision Tree Classifier 

**Decision Tree Classifier** is an Machine Learning algorithm that are designed to make decisions. As we explored above in the Regression section, decision trees operate by *splitting* the data to better evaluate the dataset. However, where this model changes in comparison to the above regression model, is that the Decision Tree *Splits the data to isolate them to each class*

> **Simplified**: Similar to above, a **Decision Tree Classifier** operates kind of like a child. It asks a question, and based on the answer, separates the data and continues down individually to each group, asking more questions. 
 - Are you wearing blue?
    - Yes means you can now be classified into blue shirts
    - No means you can now be classified into non-blue shirts

❗How do we Analyze Decision Tree Classifiers?

- As with all classifications, we can measure how well our DTCs operate based on their accuracy score.
"""

drug_df = pd.read_csv('https://raw.githubusercontent.com/H-Jan/ML_Walkthrough/main/Data/drug.csv')

drug_df.head()

drug_df.shape

drug_df.info()

pd.isnull(drug_df)

drug_df.describe()

correlation = drug_df.corr()
plt.figure(figsize=(10,10))

heatmap = sns.heatmap(correlation, annot=True, cmap='OrRd')
heatmap.set_title('Drug Heatmap', fontdict={'fontsize': 18}, pad=12);

"""Once again we are converting our values from a pandas dataframe to a numpy array so that our algorithm can break down and classify our data 



"""

X_drugs = drug_df[['age', 'sex', 'bp', 'cholesterol', 'Na_to_K']].values
X_drugs[0:5]

"""Now we convert categorical, such as sex and blood pressure features, into numerical values using pandas. This way, we are only left with numbers for our model to evaluate."""

Sex = preprocessing.LabelEncoder()
Sex.fit(['F', 'M'])
X_drugs[:,1] = Sex.transform(X_drugs[:,1])

BP = preprocessing.LabelEncoder()
BP.fit([ 'LOW', 'NORMAL', 'HIGH'])
X_drugs[:,2] = BP.transform(X_drugs[:,2])

Chol = preprocessing.LabelEncoder()
Chol.fit([ 'NORMAL', 'HIGH'])
X_drugs[:,3] = Chol.transform(X_drugs[:,3]) 

X_drugs[0:5]

"""Setting our target variable"""

y_target = drug_df["drug"]
y_target[0:5]

"""And in true tradition, we are training, testing, and splitting our data. """

X_train, X_test, y_train, y_test = train_test_split(X_drugs, y_target, test_size=0.3, random_state=42)

"""Printing the shape of X_train and y_train to ensure that dimensions match """

print(X_train.shape)
print(y_train.shape)

"""Modeling our Decision Tree Classifier"""

drug_Tree = DecisionTreeClassifier(criterion="entropy", max_depth = 4)
# it shows the default parameters
drug_Tree 

drug_Tree.fit(X_train,y_train)

"""Setting up our predictions"""

prediction_Tree = drug_Tree.predict(X_test)

"""Visually Comparing Predictions"""

print (prediction_Tree [0:5])
print (y_test [0:5])

"""Evaluation"""

print("DecisionTrees's Accuracy: ", metrics.accuracy_score(y_test, prediction_Tree))

"""**Results**: Our results yielded an accuracy of 1.0, or *perfect*.

**Or is it?**

Rarely in data science do we achieve perfection, and such a score indicates other culprits at hand, and thus requires more investigation. It is important to note: *100% accuracy isn't always good*.

##Logistic Regression

**Logistic Regression** is a common ML algorithm used to assign observations to a discrete set of classes through probability. 
And yes, the naming scheme is *deceptive* but it is indeed a classification algorithm

> **Simplified**: Logistic Regression uses a  model to predict the probability of a datapoint fitting into a certain class.

❗How do we Analyze Logistic Regression?

As with previous classification algorithms, we use the accuracy score metric to evaluate our model. However, we as well use additional metrics to understand how our model performs, including: 
- **Recall Score**

    - A metric used to calculate the True Positive Rate, or the ratio of true negative cases our model is correctly able to detect. A high Recall score usually means that for most true, positive cases, we have detected them. However, it can also mean that in our frenzy of detecting all positive cases, we also labeled negative cases as true. 

 - Calculated as True Positives / (True Positives + False Negatives)

- **Precision Score**

 - This metric determines the precision, or certainty, of our models predictions of negative cases, and is an important metric to understand how accurate we are at making good predictions.
 - Calculated as True Positives / (True Positives + False Positives)

- **F1 Score**
 - An F1 score is a classifier which helps us understand the relationship between Recall and Precision. Whereas the two metrics are usually a tradeoff, a high F1 score only occurs when both Precision and Recall are high. This essentially means that our model is very picky in classifying negative cases, and very accurate when it does classify a specific case as negative or positive.
"""

churn_df = pd.read_csv('https://raw.githubusercontent.com/H-Jan/ML_Walkthrough/main/Data/churn_Data.csv')

churn_df.head()

churn_df.shape

churn_df.info()

churn_df.describe()

correlation = churn_df.corr()
plt.figure(figsize=(10,10))

heatmap = sns.heatmap(correlation, annot=True, cmap='OrRd')
heatmap.set_title('Churn Heatmap', fontdict={'fontsize': 18}, pad=12);

"""Defining X and Y for our dataset"""

X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])

y = np.asarray(churn_df['churn'])

"""Normalization and splitting the dataset as done previously"""

X = preprocessing.StandardScaler().fit(X).transform(X)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
print ('Train set:', X_train.shape,  y_train.shape)
print ('Test set:', X_test.shape,  y_test.shape)

"""Modeling"""

LogR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)
LogR

y_prob = LogR.predict_proba(X_test)

y_predict = LogR.predict(X_test)
y_predict

"""**Confusion Matrix:** We can evaluate our model with a Confusion Matrix, which breaks down our metrics into a visualization. 

It is structured as follows:

- Top Left Quadrant = True Positive Cases
 - Indicates that our model correctly classified negative values

- Top Right Quadrant = False Positives
 - Indicates that our model incorrectly classified negative values

- Bottom Left Quadrant = False Negatives
 - Indicates that our model incorrectly classified positive values 

- Bottom Right Quadrant = True Negatives
 - Indicates that our model correctly identified positive values
 
"""

# Plot non-normalized confusion matrix
plt.figure()
plot_confusion_matrix(LogR, X_test, y_test)

"""Calculate Metrics"""

print(classification_report(y_test, y_predict))

"""**Results:** As you can see, our simple Logistic Regression Classifier was effective in determining and discerning customer churn rate with moderately high Accuracy, Precision, F1, and Recall scores.

##SVM (Support Vector Machines)

**Support Vector Machines, or SVMs** are a ML classifier used to perform linear and nonlinear classification and outlier detection by imposing a hyperplane in an N-dimensional space to classify the datapoints.

> **Simplified**: Data can be represented in multiple dimensions. SVM's essentially take higher dimensions of our data, and classify them into groups based on their attributes. 

❗How do we Analyze Support Vector Machines?

As with the above Classifications, we can use **accuracy, F1 score, precision, and recall** to judge the efficacy of our SVM
"""

cell_df = pd.read_csv('https://raw.githubusercontent.com/H-Jan/ML_Walkthrough/main/Data/cell_samples.csv')

cell_df.head()

cell_df.shape

cell_df.info()

"""We see only one feature is non-int, we will change that"""

cell_df.describe()

correlation = cell_df.corr()
plt.figure(figsize=(10,10))

heatmap = sns.heatmap(correlation, annot=True, cmap='OrRd')
heatmap.set_title('Cell Heatmap', fontdict={'fontsize': 18}, pad=12);

cell_df = cell_df[pd.to_numeric(cell_df['BareNuc'], errors='coerce').notnull()]
cell_df['BareNuc'] = cell_df['BareNuc'].astype('int')

feature_df = cell_df[['Clump', 'UnifSize', 'UnifShape', 'MargAdh', 'SingEpiSize', 'BareNuc', 'BlandChrom', 'NormNucl', 'Mit']]
X_cell = np.asarray(feature_df)
X_cell[0:5]

cell_df['Class'] = cell_df['Class'].astype('int')
y_cell = np.asarray(cell_df['Class'])
y_cell [0:5]

X_train, X_test, y_train, y_test = train_test_split(X_cell, y_cell, test_size=0.25, random_state=42)
print ('Train set:', X_train.shape,  y_train.shape)
print ('Test set:', X_test.shape,  y_test.shape)

"""We will use radial basis function kerneling type for SVC. Lets start with modeling."""

Support_VM = svm.SVC(kernel='rbf')
Support_VM.fit(X_train, y_train)

"""After being fitted, our model can be used to predict new values"""

y_predict = Support_VM.predict(X_test)
y_predict [0:5]

plt.figure()
plot_confusion_matrix(Support_VM, X_test, y_test)

print(classification_report(y_test, y_predict))

"""**Results**: Our Support Vector Machine performed extremely well, returning a 95% accuracy as well as relatively high precision, recall, and f1-score values

#Clustering

##K-Means Clustering

**K-Means Clustering** is a ML clustering algorithm similar to KNN, in that it refers to a collection of datapoints in aggregation due to certain similarities

> **Simplified:** K-Means clustering groups similar data points together and discovers underlying patterns

❗How do we Analyze K-Means Clustering?

It is somewhat difficult to discretely analyze K-Means Clustering aside from visualization and deducing patterns from it's groupings.
"""

customer_df = pd.read_csv('https://raw.githubusercontent.com/H-Jan/ML_Walkthrough/main/Data/cust_segmentation_Data.csv')

customer_df.head()

customer_df.shape

customer_df.info

pd.isnull(customer_df)

customer_df.describe()

correlation = customer_df.corr()
plt.figure(figsize=(10,10))

heatmap = sns.heatmap(correlation, annot=True, cmap='OrRd')
heatmap.set_title('Customer Heatmap', fontdict={'fontsize': 18}, pad=12);

"""First we normalize the values over standard deviation """

X_customer = customer_df.values[:,1:]
X_customer = np.nan_to_num(X_customer)

clustered_data = StandardScaler().fit_transform(X_customer)
clustered_data

"""Modeling"""

num_clusters = 3
k_means = KMeans(init = "k-means++", n_clusters = num_clusters, n_init=12)

k_means.fit(X_customer)
labels = k_means.labels_

"""Distributions of customers based on their age and income"""

area = np.pi * ( X_customer[:, 1])**2  
plt.scatter(X_customer[:, 0], X_customer[:, 3], s=area, c=labels.astype(np.float), alpha=0.5)
plt.xlabel('Age', fontsize=18)
plt.ylabel('Income', fontsize=16)

plt.show()

"""Now lets visualize our results in 3 Dimensions"""

fig = plt.figure(1, figsize=(8, 6))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)

plt.cla()
ax.set_xlabel('Education')
ax.set_ylabel('Age')
ax.set_zlabel('Income')

ax.scatter(X_customer[:, 1], X_customer[:, 0], X_customer[:, 3], c= labels.astype(np.float))

"""**Results:** K-means partitioned our customers into mutually exclusive groups, in this case, into 3 clusters. The customers in each cluster are similar to each other demographically. As a result, we can create a profile for each group, considering the common characteristics of each cluster. An example may be: 
 - Affluent, educated, old
 - Middle aged, middle income
 - Young and low income
"""